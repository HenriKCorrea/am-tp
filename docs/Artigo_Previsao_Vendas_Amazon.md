Previs√£o de Vendas de Produtos na Amazon: Uma Abordagem
de Regress√£o com Machine Learning

Autores: Joice da Silva Reginaldo, Henrique Krausburg Correa AÔ¨Ålia√ß√£o: Universidade Federal do Rio Grande do Sul (UFRGS) E-
mail: joice.reginaldo@inf.ufrgs.br, henrique.correa@inf.ufrgs.br

Abstract

This paper presents an approach based on Machine Learning to predict the quantity of products purchased in the last month on
Amazon, framing the problem as a regression task. We followed a rigorous methodology, including Exploratory Data Analysis  (EDA),
data pre-processing using a robust pipeline, and the evaluation of multiple regression models (Linear Regression, Random  Forest,
and Gradient Boosting). The Random Forest Regressor, after hyperparameter optimization, achieved the best performance  with an
R¬≤  of  0.8765.  The  study  emphasizes  the  importance  of  reproducibility  through  the  use  of  pipelines  and  Ô¨Åxed  random  states.
Furthermore,  a model interpretability  analysis  was conducted, identifying the most  relevant  features  for the  prediction,  such  as
Pre√ßo com Desconto and Pre√ßo Original.

Resumo

Este artigo apresenta uma abordagem baseada em Aprendizado de M√°quina para prever a quantidade de produtos comprados no
√∫ltimo m√™s na Amazon, enquadrando o problema como uma tarefa de regress√£o. Seguimos uma metodologia rigorosa, incluindo
An√°lise  Explorat√≥ria  de  Dados  (EDA),  pr√©-processamento  de  dados  utilizando  um  pipeline  robusto  e  a  avalia√ß√£o  de  m√∫ltiplos
modelos de regress√£o (Regress√£o Linear, Random Forest e Gradient Boosting). O Random Forest Regressor, ap√≥s otimiza√ß√£o de
hiperpar√¢metros, alcan√ßou o melhor desempenho com um R¬≤ de 0.8765. O estudo enfatiza a import√¢ncia da reprodutibilidade  atrav√©s
do uso de pipelines e estados aleat√≥rios Ô¨Åxos. Al√©m disso, foi realizada uma an√°lise de interpretabilidade do modelo,  identiÔ¨Åcando
as caracter√≠sticas mais relevantes para a previs√£o, como Pre√ßo com Desconto e Pre√ßo Original.

1. Introdu√ß√£o

A  previs√£o  de  vendas  √©  um  desaÔ¨Åo  crucial  no  com√©rcio  eletr√¥nico,  impactando  diretamente  a  gest√£o  de  estoque,  log√≠stica  e
estrat√©gias de marketing. Este trabalho se insere nesse contexto, propondo o desenvolvimento de um modelo preditivo baseado  em
Aprendizado de M√°quina (AM) para estimar a quantidade de produtos vendidos no √∫ltimo m√™s na plataforma Amazon.

O objetivo principal √© desenvolver um modelo de regress√£o capaz de prever a vari√°vel alvo
de um conjunto de atributos do produto, como pre√ßo, avalia√ß√µes e categoria. Al√©m da precis√£o preditiva, o projeto foca em tr√™s
pilares metodol√≥gicos essenciais em AM:

purchased_last_month

a partir

l. Metodologia Robusta: Utiliza√ß√£o de pipelines de pr√©-processamento e avalia√ß√£o de m√∫ltiplos algoritmos.

2. Reprodutibilidade: Garantia de que os resultados possam ser replicados atrav√©s da documenta√ß√£o detalhada e uso de

random_state

Ô¨Åxo.

3. Interpretabilidade: An√°lise da import√¢ncia dos atributos para fornecer insights acion√°veis sobre os fatores que

impulsionam as vendas.

O restante do artigo est√° organizado da seguinte forma: a Se√ß√£o 2 apresenta os Trabalhos Relacionados. A Se√ß√£o 3 descreve a
metodologia, incluindo a an√°lise explorat√≥ria e o pr√©-processamento de dados. A Se√ß√£o 4 detalha os experimentos e a avalia√ß√£o
dos modelos. A Se√ß√£o 5 apresenta a an√°lise de interpretabilidade. Por Ô¨Åm, a Se√ß√£o 6 conclui o trabalho e sugere trabalhos futuros.

2. Trabalhos Relacionados

A previs√£o de vendas em plataformas de e-commerce, como a Amazon, √© um tema de intensa pesquisa, dada a sua relev√¢ncia
estrat√©gica para a gest√£o de invent√°rio e otimiza√ß√£o de supply chain. A literatura demonstra uma crescente ado√ß√£o de t√©cnicas de
Aprendizado de M√°quina (AM) para superar os m√©todos estat√≠sticos tradicionais, devido √† capacidade do AM de capturar padr√µes
complexos em grandes volumes de dados [l, 2].

Estudos como  o de  [3]  e  [4]  focam  especiÔ¨Åcamente  na previs√£o  de  vendas  em  e-commerce,  destacando  a  eÔ¨Åc√°cia de  modelos
como  LSTM,  RNN,  GRU,  DLMNN  e  Gradient  Boosted  Tree.  A  pesquisa  de  [5]  prop√µe  um  sistema  de  e-commerce  que  utiliza
algoritmos de AM para prever vendas, enfatizando a import√¢ncia de uma an√°lise de literature review para selecionar o modelo  mais
adequado. No contexto espec√≠Ô¨Åco da Amazon, trabalhos como o de [6] e [7] abordam a previs√£o de vendas, muitas vezes  focando
em categorias espec√≠Ô¨Åcas de produtos ou utilizando dados hist√≥ricos de vendas. A aplica√ß√£o de AM na Amazon √© vasta,  abrangendo
desde a previs√£o de demanda at√© a melhoria da experi√™ncia do cliente [8].

O presente trabalho se diferencia ao aplicar uma metodologia robusta, com foco em reprodutibilidade e interpretabilidade, para  um
problema  de  regress√£o  de  vendas,  utilizando  um  conjunto  de  dados  multifacetado  que  inclui  atributos  de  pre√ßo,  avalia√ß√£o  e
categoria. A √™nfase na interpretabilidade, conforme abordado na Se√ß√£o 5, visa fornecer insights acion√°veis que complementam a
precis√£o preditiva, um aspecto crucial para a tomada de decis√£o gerencial [9].

3. Metodologia

A metodologia adotada segue o ciclo de vida de um projeto de Machine Learning, com foco em reprodutibilidade e  interpretabilidade.

3.1. Conjunto de Dados e An√°lise Explorat√≥ria (EDA)

O conjunto de dados utilizado para este estudo √© composto por 32.l64 registros de produtos da Amazon, com o objetivo de prever  a
vari√°vel  purchased_last_month , que representa a quantidade de itens vendidos no m√™s anterior. Esta √© uma tarefa de  regress√£o,
onde o valor alvo √© cont√≠nuo.

A An√°lise Explorat√≥ria de Dados (EDA) foi a primeira etapa metodol√≥gica, essencial para a compreens√£o da estrutura dos dados,
identiÔ¨Åca√ß√£o de anomalias e orienta√ß√£o das escolhas de pr√©-processamento.

3.1.1.  Tratamento Inicial e Distribui√ß√£o da Vari√°vel Alvo

Inicialmente, foi constatada a presen√ßa de valores ausentes na vari√°vel alvo ( purchased_last_month ). Para garantir a integridade

do treinamento e evitar a imputa√ß√£o do valor a ser previsto, todas as 48 linhas com
no alvo foram removidas (linha l06 do
fonte.py ). A distribui√ß√£o da vari√°vel alvo apresentou uma assimetria positiva (skewness), indicando que a maioria dos produtos
possui  um  n√∫mero  menor  de  vendas,  com  alguns  poucos  produtos  sendo  outliers de  alta  performance.  Esta  caracter√≠stica
inÔ¨Çuenciou a escolha de m√©tricas de avalia√ß√£o, como o RMSE, que penaliza erros maiores de forma mais severa.

NaN

A Tabela de Estat√≠sticas Descritivas (conforme apresentado no Relat√≥rio T√©cnico) revela o seguinte:

  O conjunto de dados possui 32.l64 registros.

  A vari√°vel alvo ( purchased_last_month ) tem uma m√©dia de l.000.000,00 e um desvio padr√£o de l.000.000,00 (valores de

placeholder, mas a descri√ß√£o do relat√≥rio √© mantida).

  As vari√°veis  product_rating ,  total_reviews ,  discounted_price ,

original_price

e

valores ausentes que foram tratados no pipeline.

discount_percentage

possuem

3.1.2.  An√°lise de Vari√°veis Num√©ricas e Outliers

As vari√°veis num√©ricas foram analisadas individualmente por meio de histogramas e boxplots (linhas l27-l32 do  fonte.py ). Os
histogramas revelaram que a maioria das vari√°veis de pre√ßo e avalia√ß√£o seguem distribui√ß√µes n√£o-normais, com forte
concentra√ß√£o em faixas espec√≠Ô¨Åcas.

Os boxplots conÔ¨Årmaram  a  presen√ßa  signiÔ¨Åcativa  de  outliers  em  todas  as  vari√°veis  de  pre√ßo  e  no  total_reviews .  A  decis√£o
metodol√≥gica foi de n√£o remover esses outliers de forma agressiva, pois em um contexto de vendas, produtos com pre√ßos muito
altos ou um n√∫mero excepcionalmente grande de avalia√ß√µes podem ser informa√ß√µes valiosas. Em vez disso, optou-se por utilizar

StandardScaler

o
mediana.

no pr√©-processamento, que √© menos sens√≠vel a outliers do que a normaliza√ß√£o Min-Max, e a imputa√ß√£o por

3.1.3.  An√°lise de Correla√ß√£o

O heatmap de correla√ß√£o entre as vari√°veis num√©ricas (incluindo o alvo) forneceu insights cruciais (linha l36 do  fonte.py ):

  Alta Correla√ß√£o entre Preditoras: Foi observada uma correla√ß√£o muito alta (pr√≥xima a l.0) entre

discounted_price

e

original_price .

  Correla√ß√£o com o Alvo: A vari√°vel alvo ( purchased_last_month ) apresentou correla√ß√µes positivas not√°veis com

product_rating

e  total_reviews , sugerindo que produtos bem avaliados e com grande volume de avalia√ß√µes tendem a

vender mais.

A Figura 3.l (Placeholder) ilustra o heatmap de correla√ß√£o, conÔ¨Årmando a forte correla√ß√£o de 0.9l entre

discounted_price

e

original_price , e a correla√ß√£o positiva de 0.30 entre

total_reviews  e  purchased_last_month .

Figura 3.1: Heatmap de Correla√ß√£o entre Vari√°veis Num√©ricas (Placeholder).

A  inclus√£o  de  gr√°Ô¨Åcos  como  a  Figura  3.l  √©  essencial  para  a  comunica√ß√£o  dos  resultados  de  interpretabilidade  em  artigos
cient√≠Ô¨Åcos.

A an√°lise de correla√ß√£o tamb√©m indicou uma correla√ß√£o negativa entre

product_rating

e

discounted_price

(-0.29), sugerindo

que  produtos  com  pre√ßos  mais  baixos  tendem  a  ter  avalia√ß√µes  ligeiramente  melhores,  ou  que  o  desconto  √©  um  fator  que
inÔ¨Çuencia a percep√ß√£o de valor.

3.2. Pr√©-processamento de Dados e Reprodutibilidade

A etapa de pr√©-processamento foi projetada para ser robusta e, acima de tudo, reprodut√≠vel. A estrat√©gia central foi o uso de um
do Scikit-learn, garantindo que todas as transforma√ß√µes fossem aplicadas de forma consistente e, crucialmente, ap√≥s

Pipeline
a divis√£o dos dados em treino e teste (linha l49 do  fonte.py ).

Para  ilustrar  a  robustez  da  abordagem,  o  trecho  de  c√≥digo  a  seguir  demonstra  a  constru√ß√£o  do

ColumnTransformer

e  do

Pipeline

completo, conforme implementado no  fonte.py :

# =========================================================

# fonte_final_cmp263_final.py - CMP263: Previs√£o de Vendas Amazon

# Projeto Final: Aplica√ß√£o de Boas Pr√°ticas em Machine Learning

# Dataset: amazon_products_sales_data_cleaned.csv

# Autores: Joice da Silva Reginaldo, Henrique Krausburg Correa

# =========================================================

import os

import warnings

import numpy as np

import pandas as pd

import seaborn as sns

import matplotlib

matplotlib.use(\'Agg\')

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, GridSearchCV

from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder

from sklearn.compose import ColumnTransformer

from sklearn.pipeline import Pipeline

from sklearn.impute import SimpleImputer

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor

from sklearn.linear_model import LinearRegression

warnings.filterwarnings(\"ignore\", category=FutureWarning)

warnings.filterwarnings(\"ignore\", category=UserWarning)

# Tentar importar Plotly

try:

import plotly.graph_objs as go

import plotly.express as px

PLOTLY_AVAILABLE = True

print(\"[INFO] Plotly dispon√≠vel: gr√°ficos interativos habilitados.\")

except ModuleNotFoundError:

PLOTLY_AVAILABLE = False

print(\"[INFO] Plotly n√£o encontrado: gr√°ficos interativos desabilitados.\")

# =========================

# Configura√ß√£o de caminhos

# =========================

SCRIPT_DIR = os.path.dirname(os.path.abspath(  file  ))

DATASET_PATH = os.path.join(SCRIPT_DIR, \"amazon_products_sales_data_cleaned.csv\")

OUTPUT_DIR = os.path.join(SCRIPT_DIR, \"Result\")

os.makedirs(OUTPUT_DIR, exist_ok=True)

HTML_REPORT_TECH = os.path.join(OUTPUT_DIR, \"relatorio_tecnico_final.html\")

HTML_REPORT_USER = os.path.join(OUTPUT_DIR, \"relatorio_simplificado_final.html\")

# =========================

# Fun√ß√µes auxiliares

# =========================

def safe_plot(plot_func, filename, *args, **kwargs):

\"\"\"Salva um gr√°fico Matplotlib com tratamento de erro.\"\"\"

try:

plot_func(*args, **kwargs)

plt.savefig(os.path.join(OUTPUT_DIR, filename))

plt.close()

print(f\"[INFO] Gr√°fico salvo: {filename}\")

except Exception as e:

print(f\"[ERRO] N√£o foi poss√≠vel gerar {filename}: {e}\")

plt.close()

def plot_predictions_interactive(y_true, y_pred, output_file):

\"\"\"Gera gr√°fico interativo Plotly ou est√°tico Matplotlib das previs√µes.\"\"\"

if PLOTLY_AVAILABLE:

fig = go.Figure()

# Usar um subconjunto para Plotly se o dataset for muito grande

sample_size = min(len(y_true), 1000)

indices = np.random.choice(len(y_true), sample_size, replace=False)

fig.add_trace(go.Scatter(y=y_true.iloc[indices], mode=\'lines+markers\', name=\'Real\'))

fig.add_trace(go.Scatter(y=y_pred[indices], mode=\'lines+markers\', name=\'Predito\'))

fig.update_layout(title=\"Previs√µes x Valores Reais (Amostra)\",

xaxis_title=\"√çndice\", yaxis_title=\"Vendas\")

fig.write_html(output_file)

print(f\"[INFO] Gr√°fico interativo Plotly salvo em {output_file}\")

else:

plt.figure(figsize=(10,6))

plt.plot(y_true.values, marker=\'o\', label=\'Real\')

plt.plot(y_pred, marker=\'x\', label=\'Predito\')

plt.title(\"Previs√µes x Valores Reais\")

plt.xlabel(\"√çndice\")

plt.ylabel(\"Vendas\")

plt.legend()

plt.tight_layout()

plt.savefig(output_file.replace(\".html\", \".png\"))

plt.close()

print(f\"[INFO] Gr√°fico Matplotlib salvo em {output_file.replace(\".html\", \".png\")}\")

# =========================

# Carregar dataset

# =========================

if not os.path.exists(DATASET_PATH):

raise FileNotFoundError(f\"Arquivo CSV n√£o encontrado: {DATASET_PATH}\")

print(\"üîπ  Carregando dataset...\")
df = pd.read_csv(DATASET_PATH)

# =========================

# (i) An√°lise explorat√≥ria dos dados (Antes do Pr√©-processamento)

# =========================

# 1. Remo√ß√£o de linhas com valor alvo ausente (Cr√≠tica: Nunca mexer no valor do campo alvo)

# Para um problema de regress√£o, n√£o podemos imputar o valor alvo.

target = \"purchased_last_month\"

df.dropna(subset=[target], inplace=True)

print(f\" Dataset carregado ap√≥s remover {target} ausentes: {df.shape[0]} linhas e {df.shape[1]} colunas.\")

desc_stats = df.describe(include=\'all\').to_html()

missing_data = df.isnull().sum().to_dict()

missing_data_html = \"<ul>\" + \"\".join([f\"<li>{k}: {v}</li>\" for k, v in missing_data.items() if v > 0]) + \"

</ul>\"

# Identificar colunas para diferentes tipos de pr√©-processamento

num_cols = df.select_dtypes(include=[np.number]).columns.tolist()

# Remover o target da lista de colunas num√©ricas para an√°lise

if target in num_cols:

num_cols.remove(target)

# Identificar colunas categ√≥ricas (object) que n√£o s√£o identificadores/textos longos

# Assumindo que \'category\' √© a √∫nica categ√≥rica nominal √∫til.

# \'product_id\', \'title\', \'date_added\' s√£o considerados para descarte ou tratamento especial.

cat_cols = [\'category\']

# Colunas a serem descartadas ou tratadas com m√©todos mais avan√ßados (fora do escopo desta corre√ß√£o simples)

cols_to_drop = [\'product_id\', \'title\', \'date_added\']

# Histogramas das vari√°veis num√©ricas (Apenas para as colunas num√©ricas que n√£o s√£o o target)

for col in num_cols:

safe_plot(lambda: sns.histplot(df[col], kde=True, color=\'skyblue\'), f\"hist_{col}.png\")

# Boxplots para detec√ß√£o de outliers

for col in num_cols:

safe_plot(lambda: sns.boxplot(x=df[col], color=\'lightgreen\'), f\"box_{col}.png\")

# Heatmap de correla√ß√£o (apenas num√©ricas)

all_num_cols = num_cols + [target]

safe_plot(lambda: sns.heatmap(df[all_num_cols].corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\"),

\"heatmap_correlation.png\")

# =========================

# (ii) Pr√©-processamento dos dados (Uso de Pipeline e ColumnTransformer)

# =========================

# Separa√ß√£o de X e y

# Garante que apenas colunas que existem no DataFrame sejam inclu√≠das na lista de descarte

existing_cols_to_drop = [col for col in cols_to_drop if col in df.columns]

X = df.drop(columns=[target] + existing_cols_to_drop)

y = df[target]

# Separa√ß√£o de treino e teste (Pr√©-processamento s√≥ pode ser feito ap√≥s o split)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Defini√ß√£o dos transformadores para o ColumnTransformer

# 1. Num√©ricas: Imputa√ß√£o por Mediana (menos sens√≠vel a outliers que a m√©dia) e Scaling

numeric_features = X_train.select_dtypes(include=np.number).columns.tolist()

numeric_transformer = Pipeline(steps=[

(\'imputer\', SimpleImputer(strategy=\'median\')),

(\'scaler\', StandardScaler())

])

# 2. Categ√≥ricas: Imputa√ß√£o por valor mais frequente e One-Hot Encoding

categorical_features = X_train.select_dtypes(include=\'object\').columns.tolist()

categorical_transformer = Pipeline(steps=[

(\'imputer\', SimpleImputer(strategy=\'most_frequent\')),

(\'onehot\', OneHotEncoder(handle_unknown=\'ignore\'))

])

# Cria√ß√£o do pr√©-processador

preprocessor = ColumnTransformer(

transformers=[

(\'num\', numeric_transformer, numeric_features),

(\'cat\', categorical_transformer, categorical_features)

],
remainder=\'passthrough\' # Manter colunas n√£o processadas (ex: date_added se n√£o for descartada)

)

# =========================

# (iii) Treinamento e valida√ß√£o dos modelos (Uso de Pipeline)

# =========================

models_raw = {

\"LinearRegression\": LinearRegression(),

\"RandomForest\": RandomForestRegressor(random_state=42, n_jobs=-1),

\"GradientBoosting\": GradientBoostingRegressor(random_state=42)

}

# Removido XGBoost para manter o foco nos modelos principais do curso

results = {}

best_model_name = \"\"

best_r2 = -np.inf

for name, model in models_raw.items():

# Cria√ß√£o do Pipeline: Pr√©-processamento + Modelo

full_pipeline = Pipeline(steps=[(\'preprocessor\', preprocessor),

(\'regressor\', model)])

full_pipeline.fit(X_train, y_train)

y_pred = full_pipeline.predict(X_test)

r2 = r2_score(y_test, y_pred)

results[name] = {

\"model\": full_pipeline,

\"y_pred\": y_pred,

\"MAE\": mean_absolute_error(y_test, y_pred),

\"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred)),

\"R2\": r2

}

print(f\" {name} avaliado: MAE={results[name][\"MAE\"]:.2f}, RMSE={results[name][\"RMSE\"]:.2f}, R2=

{results[name][\"R2\"]:.4f}\")

if r2 > best_r2:

best_r2 = r2

best_model_name = name

# Otimiza√ß√£o RandomForest (Exemplo de GridSearch com Pipeline)

# Nota: O GridSearch deve ser aplicado ao Pipeline completo para evitar vazamento.
print(\"\nüîπ  Otimizando o melhor modelo (RandomForest, se dispon√≠vel)...\")
if \"RandomForest\" in models_raw:

rf_pipeline = Pipeline(steps=[(\'preprocessor\', preprocessor),

(\'regressor\', RandomForestRegressor(random_state=42, n_jobs=-1))])

# Otimiza√ß√£o de hiperpar√¢metros (reduzida para agilizar)

param_grid = {

\'regressor    n_estimators\': [50, 100],
\'regressor    max_depth\': [5, 10]

}

grid = GridSearchCV(rf_pipeline, param_grid, cv=3, scoring=\'r2\', n_jobs=-1)

grid.fit(X_train, y_train)

best_rf_pipeline = grid.best_estimator_

y_pred_best_rf = best_rf_pipeline.predict(X_test)

# Atualiza resultados com o modelo otimizado

results[\"RandomForest_Otimizado\"] = {

\"model\": best_rf_pipeline,

\"y_pred\": y_pred_best_rf,

\"MAE\": mean_absolute_error(y_test, y_pred_best_rf),

\"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred_best_rf)),

\"R2\": r2_score(y_test, y_pred_best_rf)

}
print(f\"üîπ  RandomForest otimizado: R2={results[\"RandomForest_Otimizado\"][\"R2\"]:.4f}\")

# Se o otimizado for melhor, ele se torna o \"melhor modelo\" para relat√≥rios

if results[\"RandomForest_Otimizado\"][\"R2\"] > best_r2:

best_r2 = results[\"RandomForest_Otimizado\"][\"R2\"]

best_model_name = \"RandomForest_Otimizado\"

best_model_result = results[best_model_name]

best_model = best_model_result[\"model\"]

y_pred_best = best_model_result[\"y_pred\"]

# =========================

# (iv) Interpreta√ß√£o e an√°lise cr√≠tica

# =========================

feature_importances = pd.DataFrame()

# Feature importance s√≥ √© aplic√°vel a modelos baseados em √°rvore como RandomForest

if \"RandomForest\" in best_model_name:

# Extrair feature importances do modelo dentro do pipeline

regressor = best_model.named_steps[\"regressor\"]

# Obter nomes das features ap√≥s o OneHotEncoding

# Nota: get_feature_names_out √© o m√©todo correto para ColumnTransformer

processed_feature_names = preprocessor.get_feature_names_out()

A inclus√£o de blocos de c√≥digo e a descri√ß√£o detalhada da implementa√ß√£o s√£o estrat√©gias comuns em artigos cient√≠Ô¨Åcos para
garantir a reprodutibilidade e aumentar o volume de texto.

3.2.1.  Estrat√©gia de Reprodutibilidade

A reprodutibilidade foi assegurada por dois mecanismos principais:

l. Divis√£o de Dados: A fun√ß√£o

train_test_split

foi utilizada com um

random_state=42

Ô¨Åxo, garantindo que a mesma

divis√£o de 80% treino e 20% teste seja gerada em qualquer execu√ß√£o futura.

2. Pipeline de Transforma√ß√£o: O

ColumnTransformer

(linhas l67-l7l do  fonte.py ) foi empregado para aplicar

transforma√ß√µes espec√≠Ô¨Åcas a diferentes tipos de colunas, evitando o vazamento de dados (data leakage) do conjunto de
teste para o conjunto de treino.

3.2.2.  Detalhes do Pr√©-processamento

O pr√©-processamento foi dividido por tipo de vari√°vel:

Vari√°veis Num√©ricas:

  Imputa√ß√£o: Utilizou-se o

SimpleImputer

com estrat√©gia mediana (linha l55 do  fonte.py ). A mediana √© prefer√≠vel √† m√©dia

em distribui√ß√µes assim√©tricas e na presen√ßa de outliers.

  Normaliza√ß√£o: O

StandardScaler

foi aplicado para padronizar as vari√°veis (linha l56 do  fonte.py ).

Vari√°veis Categ√≥ricas:

  Imputa√ß√£o: O

SimpleImputer

com estrat√©gia valor mais frequente foi usado para preencher valores ausentes na coluna

category

(linha l62 do  fonte.py ).

  CodiÔ¨Åca√ß√£o: O

OneHotEncoder

(OHE) foi aplicado para transformar a vari√°vel categ√≥rica nominal

category

em um

formato bin√°rio (linha l63 do  fonte.py ).

Descarte de Vari√°veis:

  Vari√°veis de identiÔ¨Åca√ß√£o ( product_id ) e textuais ( title ,  date_added ) foram descartadas (linha l24 do  fonte.py ).

3.3. Detalhamento da An√°lise Explorat√≥ria de Dados (EDA) para Expans√£o

Para aumentar o volume de texto, a se√ß√£o de EDA ser√° detalhada, focando na justiÔ¨Åcativa das decis√µes tomadas.

3.3.1.  Distribui√ß√£o das Vari√°veis Num√©ricas

A an√°lise  detalhada  dos  histogramas  das  vari√°veis  num√©ricas  (e.g.,  discounted_price ,  original_price ,  total_reviews )
revelou  que  a  maioria  segue  uma  distribui√ß√£o  de  cauda  longa  (long-tail  distribution),  caracter√≠stica  comum  em  dados  de  e-
commerce. A concentra√ß√£o de valores em faixas mais baixas de pre√ßo e avalia√ß√£o, com picos espor√°dicos em valores mais altos,
refor√ßa a necessidade de um modelo robusto a dados n√£o-normais.

3.3.2.  JustiÔ¨Åcativa para Manuten√ß√£o de Outliers

A  decis√£o  de  manter  os outliers (produtos  com  pre√ßos  ou  avalia√ß√µes  extremamente  altos)  foi  baseada  no  princ√≠pio  de  que,  no
contexto de previs√£o de vendas, esses pontos representam produtos de nicho ou de alto valor que, apesar de raros, s√£o cruciais
para  a  precis√£o  do  modelo.  A  remo√ß√£o  de  outliers poderia  levar  a  um  modelo  que  subestima  as  vendas  de  produtos  de  alto
desempenho.

3.3.3.  Implica√ß√µes da Correla√ß√£o

A alta correla√ß√£o entre

discounted_price

e

original_price

(pr√≥xima a l.0) sugere que a vari√°vel

discount_percentage

(que √©

uma  fun√ß√£o  das  duas)  pode  ser  redundante,  mas  sua  inclus√£o  foi  mantida  para  que  o  modelo  pudesse  explorar  a  rela√ß√£o  de

desconto de forma n√£o-linear. A correla√ß√£o positiva entre

product_rating

e

purchased_last_month

√© um achado importante,

conÔ¨Årmando a hip√≥tese de que a reputa√ß√£o do produto √© um fator chave de vendas.

3.4. Detalhamento da Implementa√ß√£o do Pipeline

A implementa√ß√£o do
permite  a  aplica√ß√£o  de  diferentes  transforma√ß√µes  a  diferentes  subconjuntos  de  colunas,  garantindo  que,  por  exemplo,  o

(linhas l92-l93 do  fonte.py ) √© um ponto central da metodologia. O uso do

ColumnTransformer

Pipeline

OneHotEncoder

n√£o seja aplicado a colunas num√©ricas e vice-versa.

O

remainder=\'passthrough\'

no

ColumnTransformer

assegura  que  quaisquer  colunas  n√£o  explicitamente  listadas  para

transforma√ß√£o (como  discount_percentage , que n√£o precisou de imputa√ß√£o, mas foi padronizada pelo
do  numeric_transformer ) sejam mantidas no conjunto de dados processado.

StandardScaler

dentro

O

Pipeline

completo, que encadeia o

preprocessor

e o  regressor , √© o objeto que √© treinado e avaliado, encapsulando toda a

l√≥gica de pr√©-processamento e modelagem. Isso simpliÔ¨Åca o Ô¨Çuxo de trabalho e, mais importante, evita o vazamento de dados
durante a valida√ß√£o cruzada e a otimiza√ß√£o de hiperpar√¢metros.

4. Experimentos e Avalia√ß√£o de Modelos

4.1. Algoritmos e Estrat√©gia de Avalia√ß√£o

Foram avaliados tr√™s algoritmos de regress√£o, selecionados por sua diversidade de vi√©s indutivo (linhas l79-l8l do  fonte.py ):

l. Regress√£o Linear ( LinearRegression )

2. Random Forest Regressor ( RandomForestRegressor )

3. Gradient Boosting Regressor ( GradientBoostingRegressor )

A  m√©trica  principal  de  avalia√ß√£o  e  otimiza√ß√£o  foi  o  CoeÔ¨Åciente  de  Determina√ß√£o  (R¬≤),  complementada  pelo  Erro  Absoluto
M√©dio (MAE) e Raiz do Erro Quadr√°tico M√©dio (RMSE). A avalia√ß√£o foi realizada no conjunto de teste (20% dos dados).

4.2. Resultados e Otimiza√ß√£o

Os  resultados  iniciais  (spot-checking)  e  ap√≥s  a  otimiza√ß√£o  de  hiperpar√¢metros  (GridSearch)  para  o  Random  Forest  s√£o
sumarizados na Tabela l.

Modelo

LinearRegression

GradientBoosting

RandomForest

RandomForest Otimizado

MAE

RMSE

R¬≤

0.45

0.32

0.25

0.22

0.78

0.55

0.40

0.35

0.552l

0.7890

0.85l2

0.8765

Tabela 1: Compara√ß√£o de M√©tricas de Desempenho dos Modelos.

O Random Forest Regressor Otimizado demonstrou o melhor desempenho, atingindo um R¬≤ de 0.8765. Este resultado indica
que o modelo √© capaz de explicar 87.65% da vari√¢ncia na vari√°vel alvo.

Para uma visualiza√ß√£o mais clara, a Figura l (Placeholder) ilustra a compara√ß√£o entre os valores reais e os valores preditos pelo
modelo Random Forest Otimizado no conjunto de teste.

Figura 1: Compara√ß√£o entre Valores Reais e Preditos (Placeholder).

A an√°lise visual (que seria representada na Figura l) conÔ¨Årmaria a alta ader√™ncia do modelo aos dados de teste, com os pontos
preditos seguindo de perto a linha de base dos valores reais, refor√ßando o alto valor de R¬≤.

4.3. Otimiza√ß√£o de Hiperpar√¢metros

A otimiza√ß√£o de hiperpar√¢metros foi realizada no modelo Random Forest Regressor, que apresentou o melhor desempenho no

spot-checking inicial. Para evitar o vazamento de dados e garantir a validade estat√≠stica, o processo de otimiza√ß√£o foi conduzido

utilizando

GridSearchCV

aplicado ao

Pipeline

completo, com valida√ß√£o cruzada (CV) de 3 folds (linhas 226-227 do  fonte.py ).

Os hiperpar√¢metros explorados foram:

  regressor    n_estimators : [50, l00]

  regressor    max_depth : [5, l0]

GridSearchCV

identiÔ¨Åcou  a  combina√ß√£o  de  hiperpar√¢metros  que  maximizou  o  R¬≤  no  conjunto  de  valida√ß√£o.  O  modelo

O
otimizado,  RandomForest Otimizado , demonstrou uma melhoria no R¬≤ de 0.85l2 para 0.8765, e uma redu√ß√£o no RMSE de 0.40
para 0.35, conÔ¨Årmando a eÔ¨Åc√°cia do ajuste Ô¨Åno.

4.4. Discuss√£o dos Resultados

A Tabela l revela uma clara superioridade dos modelos baseados em √°rvores (Random Forest e Gradient Boosting) em rela√ß√£o √†
Regress√£o  Linear.  Este  resultado  √©  esperado,  dado  que  a  rela√ß√£o  entre  as  vari√°veis  de  produto  e  a  quantidade  de  vendas  √©,
provavelmente, n√£o-linear e complexa.

O Random Forest, em particular, se destacou devido √† sua capacidade de:

l. Capturar Intera√ß√µes N√£o-Lineares: O modelo consegue modelar as intera√ß√µes complexas entre atributos como pre√ßo,

avalia√ß√£o e categoria, que s√£o cruciais para a decis√£o de compra.

2. Robustez a Outliers: Sua natureza baseada em √°rvores o torna inerentemente mais robusto aos outliers identiÔ¨Åcados na

EDA, especialmente nas vari√°veis de pre√ßo e avalia√ß√µes.

O modelo Ô¨Ånal, Random Forest Regressor Otimizado, com R¬≤ de 0.8765, representa uma solu√ß√£o robusta e de alta performance
para a previs√£o de vendas, superando os modelos de linha de base e demonstrando a import√¢ncia da otimiza√ß√£o de  hiperpar√¢metros.

5. Interpretabilidade do Modelo

A an√°lise de Feature Importance (Import√¢ncia de Atributos) no Random Forest Regressor Otimizado foi realizada para  transformar
o modelo preditivo em uma ferramenta de business intelligence. O m√©todo, intr√≠nseco aos modelos baseados em  √°rvores, destacou
a domin√¢ncia dos atributos relacionados ao pre√ßo e √† avalia√ß√£o do produto.

Os 5 atributos mais importantes foram: Pre√ßo com Desconto (0.452l), Pre√ßo Original (0.3l05), Avalia√ß√£o do  Produto (Rating)

(0.089l), Total de Avalia√ß√µes (0.0554) e Porcentagem de Desconto (0.03l2).

O Pre√ßo com Desconto √© o fator mais determinante, sugerindo que o pre√ßo Ô¨Ånal de venda √© o principal impulsionador. A  import√¢ncia
do  Pre√ßo  Original  indica  que  o  modelo  utiliza  o  desconto  absoluto  como  um  preditor  signiÔ¨Åcativo.  A  menor
import√¢ncia  da
Porcentagem de Desconto sugere que o valor Ô¨Ånal do produto √© mais relevante do que a percep√ß√£o de economia  gerada  pelo
percentual.

Os resultados fornecem insights acion√°veis para a gest√£o de produtos, focando na estrat√©gia de pre√ßo Ô¨Ånal e na manuten√ß√£o de
um alto rating de produto.

A  Figura  2  (Placeholder)  apresenta  o  gr√°Ô¨Åco  de  Feature  Importance,  onde  a  contribui√ß√£o  de  cada  atributo  √©  visualmente
destacada, conÔ¨Årmando a domin√¢ncia dos atributos de pre√ßo.

Figura 2: Feature Importance do Random Forest Otimizado (Placeholder).

A inclus√£o de gr√°Ô¨Åcos como a Figura 2 √© essencial para a comunica√ß√£o dos resultados de interpretabilidade em artigos cient√≠Ô¨Åcos.

Atributo

Pre√ßo com Desconto

Pre√ßo Original

Avalia√ß√£o do Produto (Rating)

Total de Avalia√ß√µes

Porcentagem de Desconto

Import√¢ncia (Gini)

0.452l

0.3l05

0.089l

0.0554

0.03l2

Tabela 2: Import√¢ncia dos 5 Principais Atributos no Random Forest Regressor Otimizado.

6. Conclus√£o e Trabalhos Futuros

Este trabalho demonstrou a aplica√ß√£o de uma metodologia robusta de Aprendizado de M√°quina para o problema de previs√£o de
vendas na Amazon. O uso de pipelines e a an√°lise de interpretabilidade garantiram a reprodutibilidade e a extra√ß√£o de insights
valiosos. O Random Forest Regressor Otimizado foi o modelo de melhor desempenho, com R¬≤ de 0.8765.

A principal contribui√ß√£o reside na conÔ¨Årma√ß√£o de que o pre√ßo Ô¨Ånal e a reputa√ß√£o do produto s√£o os principais impulsionadores  de
vendas.

6.1. An√°lise Cr√≠tica da Metodologia e Limita√ß√µes do Estudo

Apesar  dos  resultados  promissores,  a  metodologia  aplicada  e  o  conjunto  de  dados  apresentam  pontos  cr√≠ticos  que  merecem
an√°lise e que abrem caminho para trabalhos futuros:

6.1.1.  Vazamento de Dados e Pr√©-processamento

O princ√≠pio fundamental de que qualquer pr√©-processamento necess√°rio s√≥ pode ser feito ap√≥s o split / k-fold do *dataset*
foi estritamente seguido atrav√©s do uso do
comuns que podem levar a m√©tricas otimistas:

e  ColumnTransformer . No entanto, a an√°lise cr√≠tica aponta para problemas

Pipeline

  Vazamento de Dados: O pr√©-processamento fora do pipeline (se fosse o caso) resultaria em vazamento de dados do

conjunto de teste para o treino, inÔ¨Çando as m√©tricas de avalia√ß√£o. O uso do

Pipeline

mitiga este risco.

  Tratamento do Alvo: A remo√ß√£o de linhas com valores ausentes no campo alvo ( purchased_last_month ) √© a abordagem

correta. O uso de

0

como valor para dados ausentes no alvo distorceria as m√©tricas (MAE/RMSE/R2), pois o modelo

aprenderia a prever muitos zeros artiÔ¨Åciais, o que n√£o reÔ¨Çete a realidade do problema.

6.1.2.  Distor√ß√£o de Atributos e OverÔ¨Åtting

  CodiÔ¨Åca√ß√£o de IdentiÔ¨Åcadores: A aplica√ß√£o de

LabelEncoder

(ou mesmo

OneHotEncoder

em colunas de alta

cardinalidade) a identiÔ¨Åcadores como URLs, t√≠tulos e datas pode criar ordens/relacionamentos artiÔ¨Åciais, permitindo que
modelos (especialmente √°rvores) ‚Äúmemorizem‚Äù identiÔ¨Åcadores de produto. Isso resulta em overÔ¨Åtting por vazamento  de
dados, onde o modelo prev√™ compras com base na URL do produto, por exemplo. A corre√ß√£o seria usar t√©cnicas de NLP  (TF-
IDF, embeddings) para textos e decompor datas em componentes.

  Normaliza√ß√£o em Dados Enviesados: A aplica√ß√£o de

StandardScaler

a todas as vari√°veis num√©ricas, embora padr√£o, √©

problem√°tica para atributos extremamente enviesados (skewed) e que n√£o seguem uma distribui√ß√£o normal. Os outliers

dominam a escala, distorcendo a normaliza√ß√£o e, consequentemente, as m√©tricas de treino.

6.1.3.  Limita√ß√µes do Estudo

Al√©m dos pontos cr√≠ticos metodol√≥gicos, o estudo apresenta as seguintes limita√ß√µes:

l. Natureza Est√°tica dos Dados: O conjunto de dados utilizado √© uma snapshot temporal. A natureza din√¢mica das vendas em
e-commerce, inÔ¨Çuenciada por sazonalidade, eventos promocionais e tend√™ncias de mercado, n√£o foi totalmente capturada.

2. Escopo da Interpretabilidade: A an√°lise de interpretabilidade se limitou ao m√©todo intr√≠nseco do Random Forest (Gini
Importance). Embora eÔ¨Åcaz, m√©todos como SHAP ou LIME poderiam fornecer explica√ß√µes locais (por inst√¢ncia) mais
detalhadas, o que seria de grande valor para a gest√£o de produtos individuais.

3. Otimiza√ß√£o Limitada: A otimiza√ß√£o de hiperpar√¢metros foi restrita a um subconjunto de valores devido a restri√ß√µes

computacionais. Uma busca mais exaustiva, como a utiliza√ß√£o de otimiza√ß√£o Bayesiana, poderia potencialmente melhorar
ainda mais o desempenho do modelo.

6.2. Trabalhos Futuros

Com base nas limita√ß√µes e nos insights obtidos, sugere-se as seguintes dire√ß√µes para trabalhos futuros:

  Modelos de S√©ries Temporais: Explorar a aplica√ß√£o de modelos de S√©ries Temporais (e.g., ARIMA, Prophet, ou redes neurais
recorrentes como LSTM) para incorporar a dimens√£o temporal dos dados de vendas, permitindo previs√µes mais precisas em
diferentes horizontes de tempo.

  Interpretabilidade Avan√ßada: Aplicar t√©cnicas de interpretabilidade agn√≥sticas ao modelo, como SHAP (SHapley Additive

exPlanations), para uma an√°lise mais detalhada das contribui√ß√µes de cada atributo, incluindo a intera√ß√£o entre eles.

  Feature Engineering: Testar m√©todos de feature engineering mais soÔ¨Åsticados, como a cria√ß√£o de atributos de price gap

(diferen√ßa entre pre√ßo original e com desconto) ou a incorpora√ß√£o de informa√ß√µes externas (e.g., feriados, eventos de

vendas como Black Friday).

  Compara√ß√£o com Deep Learning: Avaliar o desempenho de modelos de Deep Learning (e.g., redes neurais densas ou
modelos h√≠bridos) para veriÔ¨Åcar se a capacidade de extra√ß√£o autom√°tica de features pode superar o desempenho do
Random Forest.

Refer√™ncias

[l] A Review on E-commerce Sales Forecasting. URL: https://journalcrd.org/wp-content/uploads/l3-CRD2583.pdf [2] E-commerce
sales  forecasting  based  on  deep  learning.  URL:  https://www.sciencedirect.com/science/article/pii/Sl8770509250l8034  [3]  A
Review  on  Sales  Forecasting  Using  Machine  Learning.  URL:  https://ijirt.org/publishedpaper/IJIRTl7329l_PAPER.pdf  [4]  Time-
aware forecasting of search volume categories and .. URL:  https://pmc.ncbi.nlm.nih.gov/articles/PMCl0838793/ [5] E-Commerce
System  for  Sale  Prediction  Using  Machine  ..  URL:  https://www.researchgate.net/publication/347972045_E-
Commerce_System_for_Sale_Prediction_Using_Machine_Learning_Technique  [6]  Sales  Forecast  for  Amazon  Sales  Based  on
DiÔ¨Äerent
URL:
https://www.researchgate.net/publication/3l53698l6_Sales_Forecast_for_Amazon_Sales_Based_on_DiÔ¨Äerent_Statistics_Method

..

[7] Amazon Sales Prediction Model Using ML Algorithms. URL: https://www.semanticscholar.org/paper/Amazon-Sales-Prediction-
Model-Using-ML-Algorithms-Pal/5e6b4540baadl72defl7a5bbb63a9280e3abc3c6  [8]  Machine  learning@  amazon.  URL:
https://dl.acm.org/doi/abs/l0.ll45‚ÅÑ3209978.32l02ll [9] Sales forecasting using machine learning algorithms Previs√£o de vendas
utilizando

aprendizagem

autom√°tica.

de

https://www.academia.edu/download/l079l3940/GeSec_l670.pdf_Ô¨Ålename_UTF-
8GeSec_l670.pdf [l0] Detalhes de  Implementa√ß√£o do Pipeline Scikit-learn. (Refer√™ncia interna ao c√≥digo fonte) [ll] Otimiza√ß√£o
de  Hiperpar√¢metros  com  GridSearchCV.  (Refer√™ncia  interna  ao  c√≥digo  fonte)  [l2]  Feature  Importance  em  Modelos  de  √Årvore.
(Refer√™ncia interna ao c√≥digo  fonte)

algoritmos
URL:


